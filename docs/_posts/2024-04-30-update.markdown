---
layout: post
title:  "SAM & Ollama"
date:   2024-04-30
categories: daily log
---

### AWS - SAM

SAM is Serverless Application Model. The SAM template file is an abstraction over cloudformation template. The cli tool converts SAM template to Cloudformation template during deployment.

Deployed hello world application available [here](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started-hello-world.html#serverless-getting-started-hello-world-run) using SAM cli.

Also halfway through this [SAM workshop](https://catalog.workshops.aws/complete-aws-sam/en-US)

### Ollama

Figured out why Gemma 7b model inference didn't work with Ollama on M1 Air. It was an OOM issue with using Metal. Running the model on CPU works but it is very slow. More details [here][ollama-github-issue-4033].

```Ollama run``` downloads model weights along the lines of how docker images are pulled. Check how it works.

Terminologies to understand:

 - Tensor parallelism
 - Pipeline parallelism
 - Data parallelization
 - Model parallelization

Ollama internally uses llama-cpp.
[multi-gpu-settings-for-llama-cpp](https://github.com/ggerganov/llama.cpp/tree/master/examples/main#additional-options)

[ollama-github-issue-4033]: https://github.com/ollama/ollama/issues/4033#issuecomment-2084841685

 
